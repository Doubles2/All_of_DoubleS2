{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doubles2/All_of_DoubleS2/blob/master/AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gnPKV-b8axq",
        "colab_type": "code",
        "outputId": "27d56cde-17b0-412f-f49a-f8c371cb7c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgjwlVGrWa7",
        "colab_type": "text"
      },
      "source": [
        "data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i1kpgsjrVaV",
        "colab_type": "code",
        "outputId": "f7380972-7e19-4c2c-eee8-1f7a7a17089e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# 변수명은 X_Train, Y_Train, X_Test, Y_Test\n",
        "\n",
        "# # 아래 코드는 선택해서 업로드하는 방법\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in iploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "#구글 드라이브에서 업로드\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2qMIIVYSNiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Train = pd.read_csv('/gdrive/My Drive/Colab Notebooks/dataset/train_data.csv')\n",
        "Y_Train = pd.read_csv('/gdrive/My Drive/Colab Notebooks//dataset/train_label.csv')\n",
        "X_Test = pd.read_csv('/gdrive/My Drive/Colab Notebooks//dataset/test_data.csv')\n",
        "Y_Test = pd.read_csv('/gdrive/My Drive/Colab Notebooks//dataset/test_label.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qPG8qywLRdC",
        "colab_type": "code",
        "outputId": "020cfdc6-d9d2-4909-aacd-0375de336758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print('X_Train : ', X_Train.shape)\n",
        "print('Y_Train : ', Y_Train.shape)\n",
        "print('X_Test : ', X_Test.shape)\n",
        "print('Y_Test : ', Y_Test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_Train :  (201686, 22)\n",
            "Y_Train :  (201686, 1)\n",
            "X_Test :  (50422, 22)\n",
            "Y_Test :  (50422, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsp4HUpQKnm5",
        "colab_type": "code",
        "outputId": "345ec23a-a083-4a40-a918-27a1caea7f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "X_Train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visit_dow</th>\n",
              "      <th>visit_year</th>\n",
              "      <th>visit_month</th>\n",
              "      <th>holiday_flg</th>\n",
              "      <th>min_visitors</th>\n",
              "      <th>max_visitors</th>\n",
              "      <th>mean_visitors</th>\n",
              "      <th>median_visitors</th>\n",
              "      <th>count_visitors</th>\n",
              "      <th>air_genre_name</th>\n",
              "      <th>air_area_name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>rv_sum</th>\n",
              "      <th>rv_mean</th>\n",
              "      <th>rvd_mean</th>\n",
              "      <th>date_int</th>\n",
              "      <th>var_max_lat</th>\n",
              "      <th>var_max_long</th>\n",
              "      <th>long_plus_lat</th>\n",
              "      <th>air_store_id_label</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>2016</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>9.243902</td>\n",
              "      <td>9.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>35.646572</td>\n",
              "      <td>139.653247</td>\n",
              "      <td>13.751283</td>\n",
              "      <td>5.084251</td>\n",
              "      <td>5.500971</td>\n",
              "      <td>20161008</td>\n",
              "      <td>8.374060</td>\n",
              "      <td>4.620151</td>\n",
              "      <td>175.299819</td>\n",
              "      <td>386</td>\n",
              "      <td>0.048283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.871795</td>\n",
              "      <td>13.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>34.756950</td>\n",
              "      <td>134.841177</td>\n",
              "      <td>13.751283</td>\n",
              "      <td>5.084251</td>\n",
              "      <td>5.500971</td>\n",
              "      <td>20161004</td>\n",
              "      <td>9.263682</td>\n",
              "      <td>9.432221</td>\n",
              "      <td>169.598128</td>\n",
              "      <td>88</td>\n",
              "      <td>0.044954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2016</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>8.015152</td>\n",
              "      <td>7.5</td>\n",
              "      <td>66.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>35.699566</td>\n",
              "      <td>139.636438</td>\n",
              "      <td>13.751283</td>\n",
              "      <td>5.084251</td>\n",
              "      <td>5.500971</td>\n",
              "      <td>20160530</td>\n",
              "      <td>8.321066</td>\n",
              "      <td>4.636960</td>\n",
              "      <td>175.336004</td>\n",
              "      <td>376</td>\n",
              "      <td>0.002125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>19.984848</td>\n",
              "      <td>17.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.589216</td>\n",
              "      <td>130.392813</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>20161109</td>\n",
              "      <td>10.431416</td>\n",
              "      <td>13.880585</td>\n",
              "      <td>163.982029</td>\n",
              "      <td>822</td>\n",
              "      <td>0.082641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>26.048780</td>\n",
              "      <td>25.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>34.687697</td>\n",
              "      <td>135.495413</td>\n",
              "      <td>13.751283</td>\n",
              "      <td>5.084251</td>\n",
              "      <td>5.500971</td>\n",
              "      <td>20160706</td>\n",
              "      <td>9.332935</td>\n",
              "      <td>8.777985</td>\n",
              "      <td>170.183110</td>\n",
              "      <td>109</td>\n",
              "      <td>0.006358</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   visit_dow  visit_year  ...  air_store_id_label    weight\n",
              "0          5        2016  ...                 386  0.048283\n",
              "1          1        2016  ...                  88  0.044954\n",
              "2          0        2016  ...                 376  0.002125\n",
              "3          2        2016  ...                 822  0.082641\n",
              "4          2        2016  ...                 109  0.006358\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWpRBERKUsnt",
        "colab_type": "text"
      },
      "source": [
        "정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neXMCUjQUtfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_max_norm(mmnTable):\n",
        "  import math\n",
        "  import numpy as np\n",
        "\n",
        "  alp = math.pow(10, -8)\n",
        "  mmnTable_col = mmnTable.columns\n",
        "  for col in mmnTable_col:\n",
        "    mmnTable[col] = ( mmnTable[col] - mmnTable[col].min() ) / ( mmnTable[col].max() - mmnTable[col].min() + alp)\n",
        "  return mmnTable\n",
        "\n",
        "def z_norm(zTable):\n",
        "  import math\n",
        "  import numpy as np\n",
        "\n",
        "  alp = math.pow(10, -8)\n",
        "  zTable_col = zTable.columns\n",
        "  for col in zTable_col:\n",
        "    zTable[col] = ( zTable[col] - zTable[col].mean() ) / zTable[col].std()\n",
        "  return zTable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsgXtcuZW48d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_Train = z_norm(X_Train)\n",
        "#X_Test = z_norm(X_Test)\n",
        "X_Train = min_max_norm(X_Train)\n",
        "X_Test = min_max_norm(X_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPoeraeBW8dY",
        "colab_type": "code",
        "outputId": "903b6d17-e27f-417d-ed99-0b41d07068d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "X_Train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visit_dow</th>\n",
              "      <th>visit_year</th>\n",
              "      <th>visit_month</th>\n",
              "      <th>holiday_flg</th>\n",
              "      <th>min_visitors</th>\n",
              "      <th>max_visitors</th>\n",
              "      <th>mean_visitors</th>\n",
              "      <th>median_visitors</th>\n",
              "      <th>count_visitors</th>\n",
              "      <th>air_genre_name</th>\n",
              "      <th>air_area_name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>rv_sum</th>\n",
              "      <th>rv_mean</th>\n",
              "      <th>rvd_mean</th>\n",
              "      <th>date_int</th>\n",
              "      <th>var_max_lat</th>\n",
              "      <th>var_max_long</th>\n",
              "      <th>long_plus_lat</th>\n",
              "      <th>air_store_id_label</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.021689</td>\n",
              "      <td>0.056300</td>\n",
              "      <td>0.052980</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.225246</td>\n",
              "      <td>0.671814</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.041255</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.087879</td>\n",
              "      <td>0.774754</td>\n",
              "      <td>0.328186</td>\n",
              "      <td>0.470513</td>\n",
              "      <td>0.466184</td>\n",
              "      <td>0.071467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050228</td>\n",
              "      <td>0.108393</td>\n",
              "      <td>0.079470</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.254902</td>\n",
              "      <td>0.142939</td>\n",
              "      <td>0.329995</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.041255</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.087492</td>\n",
              "      <td>0.857061</td>\n",
              "      <td>0.670005</td>\n",
              "      <td>0.238182</td>\n",
              "      <td>0.106280</td>\n",
              "      <td>0.066540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.018265</td>\n",
              "      <td>0.047908</td>\n",
              "      <td>0.043046</td>\n",
              "      <td>0.955882</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.774510</td>\n",
              "      <td>0.230149</td>\n",
              "      <td>0.670620</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.041255</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.041566</td>\n",
              "      <td>0.769851</td>\n",
              "      <td>0.329380</td>\n",
              "      <td>0.471988</td>\n",
              "      <td>0.454106</td>\n",
              "      <td>0.003146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.073059</td>\n",
              "      <td>0.129653</td>\n",
              "      <td>0.105960</td>\n",
              "      <td>0.955882</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034902</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.116162</td>\n",
              "      <td>0.024232</td>\n",
              "      <td>0.097665</td>\n",
              "      <td>0.965098</td>\n",
              "      <td>0.985988</td>\n",
              "      <td>0.009338</td>\n",
              "      <td>0.992754</td>\n",
              "      <td>0.122323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045802</td>\n",
              "      <td>0.052511</td>\n",
              "      <td>0.171065</td>\n",
              "      <td>0.158940</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.136532</td>\n",
              "      <td>0.376468</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.041255</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.863468</td>\n",
              "      <td>0.623532</td>\n",
              "      <td>0.262018</td>\n",
              "      <td>0.131643</td>\n",
              "      <td>0.009411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   visit_dow  visit_year  ...  air_store_id_label    weight\n",
              "0   0.833333         0.0  ...            0.466184  0.071467\n",
              "1   0.166667         0.0  ...            0.106280  0.066540\n",
              "2   0.000000         0.0  ...            0.454106  0.003146\n",
              "3   0.333333         0.0  ...            0.992754  0.122323\n",
              "4   0.333333         0.0  ...            0.131643  0.009411\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0tUp4xarGbp",
        "colab_type": "text"
      },
      "source": [
        "Autoencoder patameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-xk7VLGqtYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "AE_learning_rate = 0.01\n",
        "AE_epochs = 500\n",
        "AE_batch_size = 256\n",
        "\n",
        "AE_train_flag = True # 학습시 사용될 flag\n",
        "\n",
        "# Network Parameters\n",
        "AE_n_hidden_1 = 16 # 1st layer num features\n",
        "AE_n_hidden_2 = 8 # 2nd layer num features (the latent dim)\n",
        "AE_n_input = X_Train.shape[0]\n",
        "AE_input_shape = X_Train.shape[1]\n",
        "\n",
        "iterations = int(AE_n_input / AE_batch_size)\n",
        "\n",
        "# tf Graph input (only pictures)\n",
        "X = tf.placeholder(\"float\", [None, AE_input_shape])\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([AE_input_shape, AE_n_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([AE_n_hidden_1, AE_n_hidden_2])),\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([AE_n_hidden_2, AE_n_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([AE_n_hidden_1, AE_input_shape])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([AE_n_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([AE_n_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([AE_n_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([AE_input_shape])),\n",
        "}\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    e_layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    # Encoder Hidden layer with sigmoid activation #2\n",
        "    e_layer_2 = tf.nn.relu(tf.add(tf.matmul(e_layer_1, weights['encoder_h2']),\n",
        "                                   biases['encoder_b2']))\n",
        "    return e_layer_2\n",
        "\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(x):\n",
        "    # Decoder Hidden layer with sigmoid activation #1\n",
        "    d_layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
        "                                   biases['decoder_b1']))\n",
        "    # Decoder Hidden layer with sigmoid activation #2\n",
        "    d_layer_2 = tf.nn.relu(tf.add(tf.matmul(d_layer_1, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return d_layer_2\n",
        "\n",
        "# Construct model\n",
        "encoder_op = encoder(X)\n",
        "decoder_op = decoder(encoder_op)\n",
        "\n",
        "# Prediction\n",
        "y_pred = decoder_op\n",
        "# Targets (Labels) are the input data.\n",
        "y_true = X\n",
        "\n",
        "# Define loss and optimizer, minimize the squared error\n",
        "#AE_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "AE_msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "AE_loss = tf.pow(AE_msle(y_true, y_pred), 0.5)\n",
        "#AE_loss = tf.reduce_mean(tf.pow(tf.log(y_true, name ='log_y') - tf.log(y_pred, name ='log_pred'), 2))\n",
        "AE_optimizer = tf.train.AdamOptimizer(AE_learning_rate).minimize(AE_loss)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "AE_test_loss_best = 100\n",
        "display_step = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pEqIfP0rPZJ",
        "colab_type": "text"
      },
      "source": [
        "autoencoder training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLL4KCJ8qtcL",
        "colab_type": "code",
        "outputId": "0ecfda91-4b4b-4c88-dc96-2a3ce06b87b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    if AE_train_flag: \n",
        "        # Training\n",
        "        for i in range(AE_epochs):\n",
        "            total_loss = 0 \n",
        "            for j in range(iterations):\n",
        "                randidx = np.random.randint(AE_n_input, size=AE_batch_size)                \n",
        "                batch_x = X_Train.loc[randidx, :]\n",
        "                # Run optimization op (backprop) and cost op (to get loss value)\n",
        "                _, l = sess.run([AE_optimizer, AE_loss], feed_dict={X: batch_x})\n",
        "                # Display logs per step\n",
        "                total_loss += l\n",
        "            # Testing\n",
        "            #AE_train_loss = sess.run(AE_loss, feed_dict={X: X_Train})\n",
        "            AE_train_loss = total_loss / iterations            \n",
        "            # Testing\n",
        "            AE_test_loss = sess.run(AE_loss, feed_dict={X: X_Test})\n",
        "            \n",
        "\n",
        "            if (i+1) % display_step == 0:\n",
        "                print(\" %d epoch - Autoencoder Training Loss: %.4f\" % (i, AE_train_loss))\n",
        "                print(\" %d epoch -  Autoencoder Test Loss: %.4f\" % (i, AE_test_loss))\n",
        "\n",
        "            if AE_train_loss < AE_test_loss_best :\n",
        "                AE_test_loss_best = AE_train_loss\n",
        "                X_Train_Encode = sess.run(encoder_op, feed_dict={X: X_Train})\n",
        "                X_Test_Encode = sess.run(encoder_op, feed_dict={X: X_Test})"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 24 epoch - Autoencoder Training Loss: 0.2269\n",
            " 24 epoch -  Autoencoder Test Loss: 0.2226\n",
            " 49 epoch - Autoencoder Training Loss: 0.2178\n",
            " 49 epoch -  Autoencoder Test Loss: 0.2202\n",
            " 74 epoch - Autoencoder Training Loss: 0.2172\n",
            " 74 epoch -  Autoencoder Test Loss: 0.2198\n",
            " 99 epoch - Autoencoder Training Loss: 0.2181\n",
            " 99 epoch -  Autoencoder Test Loss: 0.2202\n",
            " 124 epoch - Autoencoder Training Loss: 0.2176\n",
            " 124 epoch -  Autoencoder Test Loss: 0.2206\n",
            " 149 epoch - Autoencoder Training Loss: 0.2180\n",
            " 149 epoch -  Autoencoder Test Loss: 0.2202\n",
            " 174 epoch - Autoencoder Training Loss: 0.2172\n",
            " 174 epoch -  Autoencoder Test Loss: 0.2195\n",
            " 199 epoch - Autoencoder Training Loss: 0.2172\n",
            " 199 epoch -  Autoencoder Test Loss: 0.2192\n",
            " 224 epoch - Autoencoder Training Loss: 0.2172\n",
            " 224 epoch -  Autoencoder Test Loss: 0.2194\n",
            " 249 epoch - Autoencoder Training Loss: 0.2171\n",
            " 249 epoch -  Autoencoder Test Loss: 0.2196\n",
            " 274 epoch - Autoencoder Training Loss: 0.2170\n",
            " 274 epoch -  Autoencoder Test Loss: 0.2192\n",
            " 299 epoch - Autoencoder Training Loss: 0.2171\n",
            " 299 epoch -  Autoencoder Test Loss: 0.2192\n",
            " 324 epoch - Autoencoder Training Loss: 0.2171\n",
            " 324 epoch -  Autoencoder Test Loss: 0.2191\n",
            " 349 epoch - Autoencoder Training Loss: 0.2174\n",
            " 349 epoch -  Autoencoder Test Loss: 0.2193\n",
            " 374 epoch - Autoencoder Training Loss: 0.2172\n",
            " 374 epoch -  Autoencoder Test Loss: 0.2191\n",
            " 399 epoch - Autoencoder Training Loss: 0.2171\n",
            " 399 epoch -  Autoencoder Test Loss: 0.2193\n",
            " 424 epoch - Autoencoder Training Loss: 0.2172\n",
            " 424 epoch -  Autoencoder Test Loss: 0.2192\n",
            " 449 epoch - Autoencoder Training Loss: 0.2171\n",
            " 449 epoch -  Autoencoder Test Loss: 0.2194\n",
            " 474 epoch - Autoencoder Training Loss: 0.2169\n",
            " 474 epoch -  Autoencoder Test Loss: 0.2193\n",
            " 499 epoch - Autoencoder Training Loss: 0.2090\n",
            " 499 epoch -  Autoencoder Test Loss: 0.2114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ5_yng6foCB",
        "colab_type": "text"
      },
      "source": [
        "DNN 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1jwNeCHfnk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbcbfdd3-4962-474a-dead-098b252d60e5"
      },
      "source": [
        "# Network Parameters\n",
        "n_hidden_1 = 16 # 1st layer #\n",
        "n_hidden_2 = 16 # 2nd layer #\n",
        "#n_input    = X_Train.shape[1] # data input (# of feature)\n",
        "n_input    = AE_n_hidden_2 # data input (# of feature)\n",
        "n_classes  = 1 # total classes (1: death, 0: survive)\n",
        "\n",
        "# tf Graph input\n",
        "with tf.device('/gpu:0'):\n",
        "    x = tf.placeholder(\"float\", [None, n_input])\n",
        "    y = tf.placeholder(\"float\", [None, n_classes])\n",
        "    lr = tf.placeholder(\"float\", shape=[])\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(_X, _weights, _biases):\n",
        "    with tf.device('/gpu:0'):\n",
        "        layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
        "        #layer_2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])), 0.5)        \n",
        "        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2']))\n",
        "    return tf.matmul(layer_2, _weights['out']) + _biases['out']\n",
        "\n",
        "# Store layers weight & bias\n",
        "stddev = 0.1 # <== This greatly affects accuracy!!\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    weights = {\n",
        "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
        "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
        "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=stddev))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "print(\"Network Ready to Go!\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Ready to Go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcr3zK7T6yWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64e3f002-f3bd-4b65-f185-944869fcfbad"
      },
      "source": [
        "X_Train_Encode.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(201686, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU-MiR_9f6NM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "385a4688-ef79-47ba-aaa0-961c1daba0f8"
      },
      "source": [
        "tf.set_random_seed(0)\n",
        "# Parameters\n",
        "learning_rate_value   = 0.01\n",
        "training_epochs = 500\n",
        "batch_size      = 256\n",
        "display_step    = 25\n",
        "log_step = int(training_epochs/display_step)\n",
        "# Add ops to save and restore all the variables.\n",
        "Model_saver = tf.train.Saver(max_to_keep=None)\n",
        "\n",
        "# Construct model\n",
        "with tf.device('/gpu:0'):\n",
        "    pred = multilayer_perceptron(x, weights, biases)\n",
        "    # Define loss and optimizer    \n",
        "    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "    cost = tf.pow(msle(y, pred), 0.5)\n",
        "    #cost = tf.reduce_mean(tf.pow(y - pred, 2))\n",
        "    optm = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)    \n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "n_train = X_Train_Encode.shape[0]\n",
        "\n",
        "# Train Model\n",
        "# Launch the graph\n",
        "sess_run = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
        "sess_run.run(init)\n",
        "\n",
        "# Training cycle\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0.\n",
        "    total_batch = int(n_train/batch_size)\n",
        "    # Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "        randidx = np.random.randint(n_train, size=batch_size)\n",
        "        batch_xs = X_Train_Encode[randidx, :]\n",
        "        batch_ys = Y_Train.loc[randidx, :]\n",
        "        # Fit training using batch data\n",
        "        if epoch < 10:\n",
        "            sess_run.run(optm, feed_dict={x: batch_xs, y: batch_ys, lr: learning_rate_value})\n",
        "        elif epoch < 20:\n",
        "            sess_run.run(optm, feed_dict={x: batch_xs, y: batch_ys, lr: 0.9*learning_rate_value})\n",
        "        else:\n",
        "            sess_run.run(optm, feed_dict={x: batch_xs, y: batch_ys, lr: 0.81*learning_rate_value})\n",
        "        # Compute average loss\n",
        "        avg_cost += sess_run.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
        "        # Display logs per epoch step\n",
        "    if (epoch+1) % display_step == 0:\n",
        "        print(\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
        "        train_loss = sess_run.run(cost, feed_dict={x: batch_xs, y: batch_ys})\n",
        "        print(\" Training Loss: %.4f\" % (train_loss))\n",
        "        test_loss = sess_run.run(cost, feed_dict={x: X_Test_Encode, y: Y_Test})\n",
        "        print(\" Test Loss: %.4f\" % (test_loss))\n",
        "\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "sess_run.close()\n",
        "print(\"Session closed.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n",
            "Epoch: 024/500 cost: 0.800018800\n",
            " Training Loss: 0.8329\n",
            " Test Loss: 0.8002\n",
            "Epoch: 049/500 cost: 0.798177787\n",
            " Training Loss: 0.8396\n",
            " Test Loss: 0.7976\n",
            "Epoch: 074/500 cost: 0.800312100\n",
            " Training Loss: 0.7690\n",
            " Test Loss: 0.7981\n",
            "Epoch: 099/500 cost: 0.798942703\n",
            " Training Loss: 0.7895\n",
            " Test Loss: 0.7982\n",
            "Epoch: 124/500 cost: 0.798201693\n",
            " Training Loss: 0.7849\n",
            " Test Loss: 0.7979\n",
            "Epoch: 149/500 cost: 0.795620020\n",
            " Training Loss: 0.8352\n",
            " Test Loss: 0.7970\n",
            "Epoch: 174/500 cost: 0.797775379\n",
            " Training Loss: 0.8029\n",
            " Test Loss: 0.7970\n",
            "Epoch: 199/500 cost: 0.797584459\n",
            " Training Loss: 0.8225\n",
            " Test Loss: 0.7969\n",
            "Epoch: 224/500 cost: 0.796914688\n",
            " Training Loss: 0.7648\n",
            " Test Loss: 0.7981\n",
            "Epoch: 249/500 cost: 0.799405765\n",
            " Training Loss: 0.7664\n",
            " Test Loss: 0.7971\n",
            "Epoch: 274/500 cost: 0.797968549\n",
            " Training Loss: 0.7888\n",
            " Test Loss: 0.7970\n",
            "Epoch: 299/500 cost: 0.797215031\n",
            " Training Loss: 0.8305\n",
            " Test Loss: 0.7973\n",
            "Epoch: 324/500 cost: 0.795123790\n",
            " Training Loss: 0.7696\n",
            " Test Loss: 0.7972\n",
            "Epoch: 349/500 cost: 0.797165298\n",
            " Training Loss: 0.8228\n",
            " Test Loss: 0.7969\n",
            "Epoch: 374/500 cost: 0.797566729\n",
            " Training Loss: 0.8620\n",
            " Test Loss: 0.7978\n",
            "Epoch: 399/500 cost: 0.795857373\n",
            " Training Loss: 0.8020\n",
            " Test Loss: 0.7973\n",
            "Epoch: 424/500 cost: 0.796771744\n",
            " Training Loss: 0.7796\n",
            " Test Loss: 0.7972\n",
            "Epoch: 449/500 cost: 0.798929466\n",
            " Training Loss: 0.7893\n",
            " Test Loss: 0.7977\n",
            "Epoch: 474/500 cost: 0.796352756\n",
            " Training Loss: 0.7472\n",
            " Test Loss: 0.7967\n",
            "Epoch: 499/500 cost: 0.795521994\n",
            " Training Loss: 0.8141\n",
            " Test Loss: 0.7973\n",
            "Optimization Finished!\n",
            "Session closed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRSXt1smgAtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}